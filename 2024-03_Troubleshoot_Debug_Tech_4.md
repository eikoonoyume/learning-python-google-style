# Memory Leak Prevention
-memory leak = when chunk of memory that's no longer needed is not released when it should be freed
-leaks get larger over time
-Garbage Collector = tool in charge of freeing memory that's no longer in use, looks at variables in use and memory assigned to them, checks if there are any portions of memory that aren't being referenced
-memory profiler = tool to figure out how the memory is being used
-valgrind = memory profiler for C and C++
-measure memory use first before optimizing pieces of code
-Python: use guppy library to view and evaluate memory use by different object types

# Disk Space Management
-causes of low space: storing too much data in too little space, misusing space like keeping temp files or caching info that doesn't get cleaned up quickly enough or at all, program keeps logging error messages to system log over and over, etc
-full hard drives lead to data loss

# Network Saturation
-latency = delay between sending a byte of data from one point and receiving it on the other
-affected by physical distance between two points and how many intermediate devices there are between them
-bandwidth = how much data can be sent or received in a second, the data capacity of the connection
-transmitting small pieces of data = latency priority, aim for <50 milliseconds - 100 milliseconds (worst-case)
-transmitting large chunks of data = bandwidth priority, need as much as possible
-traffic jams = no bandwidth left for other connections, latency can increase a lot because there's not enough bandwidth to send things
-use iftop = program to check out which processes are using the network connection, shows how much data each active connection is sending over network
-more users on same network = slower data transmit
-traffic shaping = method to restrict how much each connection takes, marks the data packets sent over network with different priorities

# Dealing with Memory Leaks (from software misbehavior)
Example 1:
-use uxterm, trigger misbehavior
-scroll buffer = nifty feature that lets us scroll up and see things that we executed and their output, contents of buffer are kept in memory (long scroll buffer --> run out of memory)
-od-cx/dev/urandom = command that takes the random numbers generated by urandom device, shows them as both characters and hexadecimal numbers, this will continue to generate numbers (needs more memory)
-open top in different terminal
-press Shift M = tell top that we want to order programs by how much memory they are using
-press Control C = stop uxterm process
-in top output, column RES = dynamic memory that's preserved for specific process, column SHR = memory shared across processes, column VIRT = lists all virtual memory allocated for each process
-column RES = usually indicated problem
-close terminal to release memory that it reserved

Example 2: (script that analyzes web pages)
-use top
-program is very complex --> stop program, use memory profiler
-use simplified version of code because multi-process application is hard to use profile on
-Python = use memory profiler module (there are others)
-add "@profile" label before main function defintion to tell profiler that we want to analyze that part
-decorator = type of label used in Python to add extra behavior to functions without modifying code
-@profiler is limited to 50 articles instead of 1000s
-problem = code is storing THE WHOLE article to keep reference to it
-solution = store titles or index entries instead of whole contents

# Using Concurrency and Threading
-Python tools to streamline processing of code: threading, multiprocessing, ayncio library
-concurrency = running things in overlapping fashion
-asynchronous threading = tell OS to prioritize certain threads over others

# Concurrency
-to use concurrency = add extra code to allow running of multiple things in a sequence with overlapping time frames
-I/O-bound programming: interfacing with networks or other hardware, concurrency helps download network based content faster and more efficiently
-CPU-bound programming: program is busy processing data, concurrency helps spread heavy CPU processes across multiple processors
-risk: more coding = risk of hard-to-find errors

# Asynchronous Threading
-concurrency = stacking on top of each other
-asynchronous threading = running multiple things at same time
# create two threads
thread1 = threading.Thread(target=thread_function, args=("Thread-1",))
thread2 = threading.Thread(target=thread_function, args=("Thread-2",))
# start threads
thread1.start()
thread2.start()
# wait for threads to finish
thread1.join()
thread2.join()

print("All threads finished")
-use asyncio to stage those processes to run in order we choose
-use await command (in asyncio) inside of event loops to control processing for utmost efficiency
-useful for I/O-bound applications when you don't want to wait for response from network, no time waste!

# Checking for Network Problems
-ping server to see if it's a server related issue or network issue
-use telnet = command to server and port you're trying to reach, tells you if server is having a timeout issue(or if it's down)
-test if something's wrong on your end or receiving end
-use ipconfig /all = command to check your default gateway in Windows
-use ifconfig -a = command in Linux, shows your IP addresses and DHCP connections
-no DHCP? Renew lease (network connection between you and IP address), it DHCP is down = this is a bigger problem
-yes DHCP? No connection? test various devices to pinpoint issue
-use #arp -n = command in Linux to see list of MAC addresses for devices on network, see if device is missing a MAC address (missing = down)
-MAC addresses = identifiers for individual computers instead of a network(IP address)

# Prioritizing Work
-Eisenhower Decision Matrix = method to split tasks into urgent and important
-technical debt = pending work that accumulates when you choose a quick-and-easy solution instead of applying the sustainable long-term one, can be created by outside parties
-IT support is often interrupted during work: rotate a person to deal with interruptions or set hours for normal requests and hours for emergencies
-use time when you will not be interrupted for MOST important tasks and complex issues
-always make a to-do list
-preferable to have a ticket management system
-check urgency for all tasks, things that may cause something bad to happen if not done or affect many people should be urgent
-share list of tasks and standard of prioritization with team members to avoid doing work multiple times and having different priorities
-estimating times for tasks = "assigning rough sizes" like small, medium, large, maybe extra small and extra large if needed. DO NOT BE OPTIMISTIC. Always base it on experience (or smaller experiences added up to equal current one), this will factor in extra time for integration of all the pieces of the project and time for unknown bumps in the raod
-if a task is not important... it shouldn't be done at all
-but make sure to take breaks and work on experimental projects
-if needed, get extra help from team members or just decide something isn't really that important = communicate everything clearly to everyone about these
-if task is large or long-term, look for ways to break it down into smaller tasks
-schedule ideally ten tasks or less for each day
-consider when you're more productive during the day
-do not overschedule, emergencies and unexpected events will always arise

# Dealing with Hard Problems (DEBUGGING)
-debugging is 2x more difficult than writing program
-advice: develop code in small, digestable chucks, stop and test
-advice: keep your goal clear, have documentation
-real life: short-term solution first to get people back to work
-advice: don't be afraid to ask for help, ask someone who has done it before (save time and frustration) but DO NOT TELL THEM WHAT YOU THINK THE ROOT CAUSE IS, tell them symptoms and answer their questions
-rubber duck debugging = explaining the problem to a rubber duck, may help you realize what you're missing

# Proactive Practices (continuous integration)
-infrastructure that lets us test changes in advance is very helpful
-make sure code has good unit tests and integration tests, run them often
-continuous integration helps with running often
-have test environment to deploy new code before shipping it out in order to check software in user pov thoroughly and troubleshoot problems
-canaries = phases of software deployment = upgrade some computers first and check behavior before upgrading all computers
-make sure you can roll back to previous version
-include good debug logging in code
-have centralized logs collection (special server that gathers all logs from all servers/computers)
-have good monitoring system to catch issues early before they affect too many (ticketing system is really useful) and send automatic alerts for things like disk space
-look for automation opportunities
-write documentation, store in well-known location, have good instructions on how to solve
-think ahead about space because migrating takes time, figure out current usage and expected growth
-problem domain = complexity of given problem that one is trying to solve
-failure domain = complexity of system, describes various sub-systems that may fail
-key to preventing future breakage is to identify and manage the scope and severity of failure domain
-hypothetico-deductive method = making observations, formulating hypotheses, systematically testing those hypotheses
-common pitfalls: fixating on unrelated symptoms, confusing correlation with causation

# Virtualized Environments and Managing Change in Them
-VM = virtual machine
-change management = structured process of altering, updating, and modifying cloud-based virtual machines
-priority = minimizing disruptions, ensuring system integrity
-small changes = significant implications due to dynamic nature
-use disk snapshot feature to roll VM back to previous state OR maintain previous VM images and use them for rollback
-Packer = tool in Google Cloud Platform, tags your images with a version number and maintains them in iamge library (ex. MyApp-1.0, MyApp-1.1, MyAPP-2.0, etc
-change in a controlled and structured manner, learn how to use snapshots

# Containerized Applications: Docker
-containers = software units that allow multiple applications to run on a single host without interfering with one another
-provide flexbility to move apps between different environments and secure them in their own isolated environment
-faster and easier to deploy than VM, they are the new standard way to package and deploy applications
-microservice-based architecture = how many companies develop in-house applications, each microservice has its own container
-container image building = include suppor tfiles, libraries, data files, upload it all to registry for servers to fetch and run
-container networking = when containers are set up to discover and connect with each other, even if not publicly accessible
Example:
-kimai = time-tracking app which runs in Docker container on computer and exposes web interface on port 8080 and requires MySQL databse
-MySQL container exposes port 3306 to kimai

# Docker
-Docker = containerization platform, bundles applications and their dependencies into portable containers (simplifies deployment and ensures consistent performance across environments)
-use if application you want to run has already been packaged as containers
-use if developing a complex application across multiple teams like a microservice-based app
-use if running an application at internet scale across hundreds or thousands of servers
-don't use if you're using Kubernetes (it likes containerd instead) or if you're running a huge, monolithic legacy application written in Java or something
-Docker alternatives: Linux kernal core tech, Kubernetes, Docker Swarm

# Qwiklabs: debug and solve software problems
# list files in home directory
ls
# output: start_date_report.py
# grant executable and editable file permission
sudo chmod 777 ~/start_date_report.py
# run python program
./start_date_report.py
# enter values of year, month, day when prompt appears
# output: TypeError
# need datetime.datetime() function to get integer types, not string types
# use nano editor
nano ~/start_date_report.py
# search for get_start_date() function, change the inputs into integer types
def get_start_date():
    """Interactively get the start date to query for."""


    print()
    print('Getting the first start date to query for.')
    print()
    print('The date must be greater than Jan 1st, 2018')
    year = int(input('Enter a value for the year: '))
    month = int(input('Enter a value for the month: '))
    day = int(input('Enter a value for the day: '))
    print()
# save and exit
# run
./start_date_report.py

# improve speed of file processing
# debug first, find bottlenecks
# check execution time, add prefix "time" and run script
time ./test.py
# use nano editor
nano ~/start_date_report.py
# find get_same_or_newer() function, modify output to be used for various dates
# make sure the file is only downloaded once from URL
# preprocess the file so that we don't have to repeat it: either create a dictionary with start dates or sort data by start_date and go by date by date

#!/usr/bin/env python3
import csv
import datetime
import requests


FILE_URL="http://marga.com.ar/employees-with-date.csv"


def get_start_date():
    """Interactively get the start date to query for."""


    print()
    print('Getting the first start date to query for.')
    print()
    print('The date must be greater than Jan 1st, 2018')
    year = int(input('Enter a value for the year: '))
    month = int(input('Enter a value for the month: '))
    day = int(input('Enter a value for the day: '))
    print()


    return datetime.datetime(year, month, day)


def get_file_lines(url):
    """Returns the lines contained in the file at the given URL"""


    # Download the file over the internet
    response = requests.get(url, stream=True)


    # Decode all lines into strings
    lines = []
    for line in response.iter_lines():
        lines.append(line.decode("UTF-8"))
    return lines

# add this line: data = get_file_lines(FILE_URL)
data = get_file_lines(FILE_URL)
def get_same_or_newer(start_date,data): # add data here in parameter
    """Returns the employees that started on the given date, or the closest one."""
    #Genera un csv
    reader = csv.reader(data[1:])
    #reader = data
    # We want all employees that started at the same date or the closest newer
    # date. To calculate that, we go through all the data and find the
    # employees that started on the smallest date that's equal or bigger than
    # the given start date.
    min_date = datetime.datetime.today()
    min_date_employees = []
    for row in reader:
        row_date = datetime.datetime.strptime(row[3], '%Y-%m-%d')


        # If this date is smaller than the one we're looking for,
        # we skip this row
        if row_date < start_date:
            continue
        # If this date is smaller than the current minimum,
        # we pick it as the new minimum, resetting the list of
        # employees at the minimal date.
        if row_date < min_date:
            min_date = row_date
            min_date_employees = []
        # If this date is smaller than the current minimum,
        # we pick it as the new minimum, resetting the list of
        if row_date == min_date:
            min_date_employees.append("{} {}".format(row[0], row[1]))
    return min_date, min_date_employees


def list_newer(start_date):
    data = get_file_lines(FILE_URL # add this line here
    while start_date < datetime.datetime.today():
        start_date, employees = get_same_or_newer(start_date,data) # add data in parameter
        print("Started on {}: {}".format(start_date.strftime("%b %d, %Y"), employees))


        # Now move the date to the next one
        start_date = start_date + datetime.timedelta(days=1)


def main():
    start_date = get_start_date()
    list_newer(start_date)


if __name__ == "__main__":
    main()
# save and exit
# run
./start_date_report.py
# answer prompts
# finished
